# AI Evaluation Demo

This project demonstrates how I approach LLM quality analysis — from collecting model outputs to labeling and scoring responses for correctness, clarity, and appropriateness. 

### What’s Inside
- `prompts/`: Example questions used to test model performance on educational content.
- `evaluations/`: Raw and labeled model responses.
- `notebooks/`: Python notebook that analyzes model quality, accuracy, and consistency.

### Tools Used
- Python
- Pandas
- JSON/CSV
- OpenAI API (or mock outputs)
